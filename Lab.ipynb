{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def prepare_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    if 'LABEL' in df.columns:\n",
    "        X, y = df[['RESPONSE','LABEL']].values.T\n",
    "        return X, y\n",
    "    else:\n",
    "        return df['RESPONSE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.id import Indonesian\n",
    "from spell import correction\n",
    "\n",
    "nlp = Indonesian()\n",
    "\n",
    "def tokenizer(text, with_correction=False):\n",
    "    if with_correction:\n",
    "        text = ' '.join([correction(token) for token in text.split(' ')])\n",
    "    return [token.lemma_ for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv('https://raw.githubusercontent.com/masdevid/ID-Stopwords/master/id.stopwords.02.01.2016.txt', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_05463b10_c680_11e9_b52e_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >test_precision</th>        <th class=\"col_heading level0 col1\" >test_recall</th>        <th class=\"col_heading level0 col2\" >test_f1</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row0_col0\" class=\"data row0 col0\" >500.00%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row0_col1\" class=\"data row0 col1\" >500.00%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row0_col2\" class=\"data row0 col2\" >500.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row1_col0\" class=\"data row1 col0\" >68.58%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row1_col1\" class=\"data row1 col1\" >76.20%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row1_col2\" class=\"data row1 col2\" >72.12%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row2_col0\" class=\"data row2 col0\" >3.55%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row2_col1\" class=\"data row2 col1\" >4.62%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row2_col2\" class=\"data row2 col2\" >3.19%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row3_col0\" class=\"data row3 col0\" >64.86%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row3_col1\" class=\"data row3 col1\" >70.59%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row3_col2\" class=\"data row3 col2\" >67.61%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row4_col0\" class=\"data row4 col0\" >65.00%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row4_col1\" class=\"data row4 col1\" >72.73%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row4_col2\" class=\"data row4 col2\" >70.27%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row5_col0\" class=\"data row5 col0\" >69.23%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row5_col1\" class=\"data row5 col1\" >76.47%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row5_col2\" class=\"data row5 col2\" >72.73%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row6_col0\" class=\"data row6 col0\" >71.05%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row6_col1\" class=\"data row6 col1\" >79.41%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row6_col2\" class=\"data row6 col2\" >75.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05463b10_c680_11e9_b52e_acde48001122level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row7_col0\" class=\"data row7 col0\" >72.73%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row7_col1\" class=\"data row7 col1\" >81.82%</td>\n",
       "                        <td id=\"T_05463b10_c680_11e9_b52e_acde48001122row7_col2\" class=\"data row7 col2\" >75.00%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1317d78d0>"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support, make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf = make_pipeline(\n",
    "    make_pipeline(\n",
    "        CountVectorizer(tokenizer=tokenizer),\n",
    "        TruncatedSVD(100, random_state=42),\n",
    "    ),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "#     GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    ")\n",
    "\n",
    "# X, y = prepare_data('Data A/data_train_A.csv')\n",
    "X, y = prepare_data('Data B/data_train_B.csv')\n",
    "y = y.astype(int)\n",
    "pd.DataFrame(\n",
    "    cross_validate(clf, X, y, scoring=['precision', 'recall', 'f1'], cv=5, return_train_score=False)\n",
    ")[['test_precision','test_recall','test_f1']].describe().style.format('{:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A, y_A = prepare_data('Data A/data_train_A.csv')\n",
    "X_B, y_B = prepare_data('Data B/data_train_B.csv')\n",
    "\n",
    "y_A = y_A.astype(int)\n",
    "y_B = y_B.astype(int)\n",
    "\n",
    "dev_A = pd.read_csv('Data A/data_dev_A.csv')\n",
    "dev_B = pd.read_csv('Data B/data_dev_B.csv')\n",
    "\n",
    "X_dev_A = dev_A['RESPONSE'].values\n",
    "X_dev_B = dev_B['RESPONSE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = make_pipeline(\n",
    "    CountVectorizer(tokenizer=tokenizer),\n",
    "    TruncatedSVD(100, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=42)\n",
    ")\n",
    "model_B = make_pipeline(\n",
    "    CountVectorizer(tokenizer=tokenizer),\n",
    "    TruncatedSVD(100, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=250, random_state=42)\n",
    ")\n",
    "\n",
    "for X, y, X_test, y_test, clf in zip([X_A, X_B], [y_A, y_B], [X_dev_A, X_dev_B], [dev_A, dev_B], [model_A, model_B]):\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_test['LABEL'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    dev_A[['RES_ID','LABEL']],\n",
    "    dev_B[['RES_ID','LABEL']]\n",
    "]).to_json('dev_{}.json'.format(pd.Timestamp.today().strftime('%Y%m%d')), orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
